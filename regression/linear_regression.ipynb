{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb89b79-7240-41d0-812d-836a8981300c",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
    "</script>\n",
    "<script type=\"text/x-mathjax-config\">\n",
    " MathJax.Hub.Config({\n",
    " tex2jax: {\n",
    " inlineMath: [['$', '$'] ],\n",
    " displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
    " }\n",
    " });\n",
    "</script>\n",
    "\n",
    "# 線形回帰\n",
    "\n",
    "## 線形回帰の式\n",
    "\n",
    "$$ \\hat{y} = W_0 + W_1^T\\times{X} $$\n",
    "\n",
    "## 損失関数\n",
    "$$ Loss = \\frac{1}{m}\\sum_{i = 1}^m (y_i - \\hat{y_i})^2 $$\n",
    "\n",
    "この損失関数が最小値となる$W_1$(係数)を求める.その求める手法が$\\textbf{最小二乗法}$である.  \n",
    "最小二乗法を解く方法は2つある.  \n",
    "- 最急降下方\n",
    "- 正規方程式\n",
    "\n",
    "最急降下法での損失関数は$L(θ_0, θ_1) = \\frac{1}{m}\\sum_{i = 1}^m (y_i - (θ_0 + θ_1x_i))^2 $と示す.  \n",
    "最急降下法では損失関数が最小になる方向にパラメータ$θ$を動かしていき、最適解を得る手法である.  \n",
    "各パラメータの更新式は以下の通りである.($α$:学習率)\n",
    "- $ θ_0 := θ_0 - α\\frac{\\partial}{\\partial θ_0}L(θ_0, θ_1) $\n",
    "- $ θ_0 := θ_1 - α\\frac{\\partial}{\\partial θ_1}L(θ_0, θ_1) $  \n",
    "\n",
    "パラメータの数が多くなれば、上記のような式が$θ_2, θ_3$と増えていくイメージ. またこれらの式は同時に更新されていく.  \n",
    "ここで、$\\frac{\\partial}{\\partial θ_0}L(θ_0, θ_1)$や$\\frac{\\partial}{\\partial θ_1}L(θ_0, θ_1)$について解いてみる.  \n",
    "- $\\frac{\\partial}{\\partial θ_0}L(θ_0, θ_1) = - \\frac{2}{m}\\sum_{i = 1}^m(y_i - (θ_0 + θ_1x_i)) = \\frac{2}{m}\\sum_{i = 1}^m(θ_0 + θ_1x_i - y_i)$\n",
    "- $\\frac{\\partial}{\\partial θ_1}L(θ_0, θ_1) = - \\frac{2}{m}\\sum_{i = 1}^m(y_i - (θ_0 + θ_1x_i))x_i = \\frac{2}{m}\\sum_{i = 1}^m(θ_0 + θ_1x_i - y_i)x_i$\n",
    "\n",
    "このようにパラメータを更新していき、損失関数が最小になるところを探していくのが最急降下法である。  \n",
    "\n",
    "次に正規方程式の解法を示す.  \n",
    "正規方程式でも解くべき損失関数は同じで、$L(θ_0, θ_1) = \\frac{1}{m}\\sum_{i = 1}^m (y_i - (θ_0 + θ_1x_i))^2 $である.  \n",
    "ただし、正規方程式では行列計算によって解く.  \n",
    "予測式は$\\hat{y} = θ_0 + θ_1x_{i1} + θ_2x_{i2} + ・・・ + θ_nx_{in}$と表される. (nは特徴量の数. iは各特徴量の何番目のデータかを示している添字)  \n",
    "これを行列で表すと$\\hat{Y} = Xθ$となる. ($\\hat{Y}$はm×1の行列. $X$はm×nの行列. $θ$はn×1の行列.)  \n",
    "損失関数では$L(θ) = \\frac{1}{m}\\sum_{i = 1}^m (Y - Xθ)^2 $となる.\n",
    "この式を以下の様に解く.  \n",
    "$(Y - Xθ)^2 = (Y - Xθ)^T(Y - Xθ) = (Y^T - θ^TX^T)(Y - Xθ) = Y^TY - 2θ^TX^TY + θ^TX^TXθ $  \n",
    "$Y^TY - 2θ^TX^TY + θ^TX^TXθ$を$θ$で偏微分して0になるなるθが最適解となる.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e5384-e2ff-4e65-8f86-652e305af34a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
